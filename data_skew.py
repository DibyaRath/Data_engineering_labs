üí° Day 6 ‚Äì PySpark Scenario-Based Interview Question

One partition working 30√ó harder than the rest ‚Äî that‚Äôs how jobs collapse.

Your cluster looks healthy.
CPU is fine.
Memory is fine.

Yet one task keeps running.

Stage stuck at 99%.

This is how data skew kills distributed systems.

Scenario:

You are a Senior Data Engineer at a global e-commerce company.

‚Ä¢ 4TB fact table
‚Ä¢ 22GB dimension table
‚Ä¢ Join on user_id
‚Ä¢ Aggregation after join
‚Ä¢ SLA: 40 minutes

After one week:

‚Ä¢ Runtime jumps 28 min ‚Üí 2 hours
‚Ä¢ Few tasks run 30√ó longer
‚Ä¢ Shuffle spill spikes
‚Ä¢ High GC
‚Ä¢ No code changes

Questions:

1 - Why are only a few tasks slow?
2 - What is happening inside shuffle?
3 - Did Spark change join strategy?
4 - How do you prove skew?
5 - How do you fix it safely?
6 - How do you prevent recurrence?

Answers 
1Ô∏è‚É£ Why Are Only a Few Tasks Slow?

Because partition sizes are uneven.

Spark distributes data by hash(user_id).
If some user_ids dominate volume ‚Üí one partition becomes huge.

That task:

‚Ä¢ Processes more data
‚Ä¢ Spills more
‚Ä¢ Runs longer

Parallel system becomes partially serialized.

2Ô∏è‚É£ What Is Happening Inside Shuffle?

During SortMergeJoin:

‚Ä¢ Map side writes shuffle files
‚Ä¢ Data hashed into partitions
‚Ä¢ Reducers fetch relevant partitions
‚Ä¢ Sorting happens in memory

If one partition is massive ‚Üí spill ‚Üí disk I/O ‚Üí GC pressure ‚Üí slowdown.

Inspect plan:

fact_df.join(dim_df, "user_id").explain("extended")

Look for:
SortMergeJoin + Exchange operators.

3Ô∏è‚É£ Did Spark Change Join Strategy?

Possibly.

If dimension table grew beyond broadcast threshold,
Spark switches from BroadcastHashJoin ‚Üí SortMergeJoin.

Check threshold:

spark.conf.get("spark.sql.autoBroadcastJoinThreshold")

Force broadcast (if safe):

from pyspark.sql.functions import broadcast

fact_df.join(broadcast(dim_df), "user_id")
4Ô∏è‚É£ How Do You Prove Skew?

Measure key distribution.

from pyspark.sql.functions import desc

fact_df.groupBy("user_id") \
    .count() \
    .orderBy(desc("count")) \
    .show(20)

If top keys dominate ‚Üí skew confirmed.

Also inspect Spark UI:
Uneven task durations = skew symptom.

5Ô∏è‚É£ How Do You Fix It Safely?

Option A: Enable AQE

spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

Option B: Salting

from pyspark.sql.functions import rand, lit

salted_fact = fact_df.withColumn("salt", (rand()*10).cast("int"))
salted_dim = dim_df.withColumn("salt", lit(0))

salted_fact.join(salted_dim, ["user_id", "salt"])

Option C: Pre-aggregate before join.

6Ô∏è‚É£ How Do You Prevent Recurrence?

‚Ä¢ Monitor skew metrics in production
‚Ä¢ Track top key distribution
‚Ä¢ Partition based on composite keys
‚Ä¢ Tune shuffle partitions properly

spark.conf.set("spark.sql.shuffle.partitions", 4000)

Partition math:
Shuffle size √∑ 256MB ‚âà partition count.

Skew prevention is architectural, not reactive.


# ==========================================
# Day 6 ‚Äì Data Skew Detection & Optimization
# ==========================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, broadcast, rand, lit

# ------------------------------------------
# Create Spark Session
# ------------------------------------------
spark = SparkSession.builder \
    .appName("Day6_DataSkew_Optimization") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.skewJoin.enabled", "true") \
    .getOrCreate()

# ------------------------------------------
# Example DataFrames (Replace with actual sources)
# ------------------------------------------
fact_df = spark.read.parquet("path/to/fact_table")
dim_df = spark.read.parquet("path/to/dimension_table")

# ==========================================
# 1Ô∏è‚É£ Detect Data Skew
# ==========================================

skew_df = (
    fact_df
    .groupBy("user_id")
    .count()
    .orderBy(desc("count"))
)

skew_df.show(20)

# ==========================================
# 2Ô∏è‚É£ Inspect Physical Plan
# ==========================================

fact_df.join(dim_df, "user_id").explain("extended")

# ==========================================
# 3Ô∏è‚É£ Check / Adjust Broadcast Threshold
# ==========================================

print("Current Broadcast Threshold:",
      spark.conf.get("spark.sql.autoBroadcastJoinThreshold"))

spark.conf.set(
    "spark.sql.autoBroadcastJoinThreshold",
    100 * 1024 * 1024  # 100MB
)

# Force Broadcast Join (if safe)
optimized_join = fact_df.join(
    broadcast(dim_df),
    "user_id"
)

# ==========================================
# 4Ô∏è‚É£ Enable Adaptive Query Execution (AQE)
# ==========================================

spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# ==========================================
# 5Ô∏è‚É£ Salting Technique for Heavy Keys
# ==========================================

salted_fact = fact_df.withColumn(
    "salt",
    (rand() * 10).cast("int")
)

salted_dim = dim_df.withColumn(
    "salt",
    lit(0)
)

salted_join = salted_fact.join(
    salted_dim,
    ["user_id", "salt"]
)

# ==========================================
# 6Ô∏è‚É£ Tune Shuffle Partitions
# ==========================================

spark.conf.set("spark.sql.shuffle.partitions", 4000)

# ==========================================
# 7Ô∏è‚É£ Write Optimized Output
# ==========================================

optimized_join.repartition("event_date") \
    .write \
    .mode("overwrite") \
    .partitionBy("event_date") \
    .parquet("path/to/output/")


      

ðŸ’¡ Day 11 â€“ PySpark Scenario-Based Interview Question

Your streaming job didnâ€™t crash.
It silently corrupted your aggregates.

No failure.
No OOM.
No red alerts.

Just wrong numbers in production.

And nobody noticed for 3 days.

ðŸ”¥ Scenario

Youâ€™re a Senior Data Engineer at a global payments platform.

You run a Structured Streaming job that:

â€¢ Ingests Kafka transactions (~120K events/sec)
â€¢ Deduplicates on transaction_id
â€¢ Aggregates 1-hour rolling fraud metrics per merchant_id
â€¢ Writes to Delta (exactly-once required)
â€¢ SLA: < 2 minutes

After a broker rebalance + network glitch:

â€¢ Some metrics doubled
â€¢ Some windows missing data
â€¢ No job failure
â€¢ Checkpoint intact

Business reports fraud spike.

Pipeline â€œran successfully.â€

ðŸ“¦ Input Dataset (Kafka â†’ Parsed Schema)

transaction_id (string)
merchant_id (string)
user_id (string)
amount (double)
event_time (timestamp)
ingestion_time (timestamp)

ðŸŽ¯ 5 Core Questions

1ï¸âƒ£ Why did aggregates double without failure?
2ï¸âƒ£ Why didnâ€™t checkpoint protect from duplication?
3ï¸âƒ£ How does watermark interact with reprocessing?
4ï¸âƒ£ How do you design idempotent streaming writes?
5ï¸âƒ£ How do you validate correctness in streaming systems?



1ï¸âƒ£ Why aggregates doubled?

Kafka rebalance caused offset replay.
Without deduplication â†’ same transaction processed twice.

Checkpoint tracks offsets.
It does NOT guarantee event uniqueness.

2ï¸âƒ£ Why checkpoint didnâ€™t save you?

Checkpoint ensures:

â€¢ Offset progress
â€¢ State recovery

It does NOT prevent:
â€¢ Duplicate event delivery
â€¢ Producer retries
â€¢ Rebalanced offset replay

Streaming is at-least-once by default.

3ï¸âƒ£ Watermark Interaction

Watermark controls state eviction.

If late data re-enters before watermark expires:
It re-updates window.

If watermark too relaxed:
State grows.
Reprocessing amplifies.

4ï¸âƒ£ Idempotent Write Design

Never rely on append mode in critical metrics.

Use:

â€¢ foreachBatch
â€¢ Deterministic MERGE
â€¢ Window start as unique key

This makes writes safe across retries.

5ï¸âƒ£ Validation Strategy

Add validation layer:

validation_df = fraud_metrics \
    .groupBy("merchant_id") \
    .agg(sum("txn_count").alias("total_txn"))

validation_df.write.mode("overwrite").save("/mnt/audit/check")

# ================================================
# Day 11 â€“ Rare Streaming Duplication Scenario
# ================================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import DeltaTable

spark = SparkSession.builder \
    .appName("Day11_Streaming_Idempotent_Design") \
    .config("spark.sql.shuffle.partitions", 800) \
    .config("spark.sql.streaming.stateStore.providerClass",
            "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider") \
    .getOrCreate()

# -----------------------------------------------
# 1ï¸âƒ£ Read Kafka Stream
# -----------------------------------------------

raw_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "broker:9092") \
    .option("subscribe", "transactions") \
    .option("startingOffsets", "latest") \
    .load()

schema = StructType([
    StructField("transaction_id", StringType()),
    StructField("merchant_id", StringType()),
    StructField("user_id", StringType()),
    StructField("amount", DoubleType()),
    StructField("event_time", TimestampType()),
    StructField("ingestion_time", TimestampType())
])

parsed_stream = raw_stream \
    .selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# -----------------------------------------------
# 2ï¸âƒ£ Deduplicate Safely (Stateful)
# -----------------------------------------------
# Problem:
# If Kafka replays offsets after rebalance,
# duplicates enter stream even with checkpoint intact.

dedup_stream = parsed_stream \
    .withWatermark("event_time", "2 hours") \
    .dropDuplicates(["transaction_id"])

# -----------------------------------------------
# 3ï¸âƒ£ Rolling Fraud Window Aggregation
# -----------------------------------------------

fraud_metrics = dedup_stream \
    .groupBy(
        window(col("event_time"), "1 hour"),
        col("merchant_id")
    ) \
    .agg(
        count("*").alias("txn_count"),
        sum("amount").alias("total_amount"),
        avg("amount").alias("avg_amount")
    )

# -----------------------------------------------
# 4ï¸âƒ£ Idempotent Write Using MERGE
# -----------------------------------------------
# Why?
# Append mode can double-write during retries.
# We need deterministic upsert logic.

def upsert_to_delta(batch_df, batch_id):

    delta_path = "/mnt/delta/fraud_metrics"

    if not DeltaTable.isDeltaTable(spark, delta_path):
        batch_df.write.format("delta") \
            .mode("overwrite") \
            .save(delta_path)
        return

    delta_table = DeltaTable.forPath(spark, delta_path)

    delta_table.alias("target") \
        .merge(
            batch_df.alias("source"),
            """
            target.merchant_id = source.merchant_id
            AND target.window.start = source.window.start
            """
        ) \
        .whenMatchedUpdate(set={
            "txn_count": "source.txn_count",
            "total_amount": "source.total_amount",
            "avg_amount": "source.avg_amount"
        }) \
        .whenNotMatchedInsertAll() \
        .execute()

# -----------------------------------------------
# 5ï¸âƒ£ Streaming Write with ForeachBatch
# -----------------------------------------------

query = fraud_metrics.writeStream \
    .foreachBatch(upsert_to_delta) \
    .option("checkpointLocation", "/mnt/checkpoints/fraud_metrics") \
    .outputMode("update") \
    .start()

query.awaitTermination()

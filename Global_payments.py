Scenario - 

You are a Senior Data Engineer (L5/L6) at a global payments company.

You built a real-time fraud detection feature pipeline.

Scale:

â€¢ 180K events/sec peak
â€¢ 12 TB/day ingestion
â€¢ 14-day rolling window
â€¢ Aggregation by user_id
â€¢ Exactly-once requirement
â€¢ SLA: < 60 seconds freshness

After 4 days in production:

â€¢ Executors crash with OOM
â€¢ Checkpoint directory grows rapidly
â€¢ Restart takes 45 minutes
â€¢ Latency increases gradually each day
â€¢ No code change


ðŸŽ¯ Core Questions

1 - What is Spark storing in the state store?
2 - Why does state grow daily?
3 - What does watermark actually do?
4 - Why is restart slow?
5 - Why does checkpoint explode?
6 - How do you make state bounded?
7 - How do you scale 5Ã— safely?
8 - How do you ensure exactly-once semantics?

ðŸ§  Answers
1ï¸âƒ£ What Is Stored in State Store?

Spark stores:

â€¢ Key (user_id + window)
â€¢ Aggregation buffers
â€¢ Offset metadata

It does NOT store raw events.
It stores intermediate aggregation state.

If 50M active users Ã— 14-day window
State size explodes.

2ï¸âƒ£ Why State Grows

Because:

â€¢ Late data extends window
â€¢ Watermark too relaxed
â€¢ Sliding windows overlap
â€¢ High cardinality keys
â€¢ No aggressive eviction

State grows linearly unless bounded.

3ï¸âƒ£ What Watermark Actually Does

Watermark defines:

â€œHow late can data arrive before eviction?â€

Without watermark â†’ state never evicts.
Incorrect watermark â†’ data loss risk.

4ï¸âƒ£ Why Restart Is Slow

On restart:

â€¢ State store reload
â€¢ Aggregation buffer reconstruction
â€¢ Checkpoint metadata replay

Large checkpoint = long recovery time.

5ï¸âƒ£ Why Checkpoint Explodes

â€¢ Every micro-batch writes metadata
â€¢ State updates persist incrementally
â€¢ Many small files accumulate
â€¢ No compaction strategy

6ï¸âƒ£ How To Bound State

â€¢ Aggressive watermark
â€¢ Reduce window size
â€¢ Pre-aggregate before window
â€¢ Avoid unnecessary high-cardinality keys
â€¢ Use session window if applicable

7ï¸âƒ£ Exactly-Once Guarantee

Spark ensures:

â€¢ Offset tracking
â€¢ Atomic commits
â€¢ Idempotent sink writes

Delta Lake strongly recommended.



from pyspark.sql import SparkSession
from pyspark.sql.functions import col, window

spark = SparkSession.builder \
    .appName("Streaming_State_Debug") \
    .config("spark.sql.streaming.stateStore.providerClass",
            "org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider") \
    .getOrCreate()

# Read streaming data
stream_df = spark.readStream \
    .format("delta") \
    .load("/mnt/events")

# Apply watermark and window aggregation
aggregated_stream = (
    stream_df
    .withWatermark("event_timestamp", "2 days")
    .groupBy(
        col("user_id"),
        window(col("event_timestamp"), "14 days", "1 day")
    )
    .count()
)

# Tune shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", 2000)

# Control state cleanup interval
spark.conf.set("spark.sql.streaming.stateStore.maintenanceInterval", "60s")

# Exactly-once sink using Delta
query = aggregated_stream.writeStream \
    .format("delta") \
    .option("checkpointLocation", "/mnt/checkpoints/fraud_pipeline") \
    .outputMode("append") \
    .start("/mnt/output/fraud_features")

query.awaitTermination()
